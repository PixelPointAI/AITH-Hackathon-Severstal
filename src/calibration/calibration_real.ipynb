{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Калибровка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Константы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем названия камер местами, для более удобного расчета, то есть теперь:  \n",
    "_cam1_ - левая камера  \n",
    "_cam2_ - правая камера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_MARKERS_DIST = 33e-3 / 6.0  # real world distance between markers in m\n",
    "M_MARKERS_DIST = 67e-3 / 6.0  # real world distance between markers in m\n",
    "GRID_SHAPE = (7, 7)\n",
    "camera_params = {\n",
    "    'focal_length_m': 80e-3,\n",
    "    'img_resolution': (5120, 4096),\n",
    "    'pixel_sz_x': 4.5e-6,  # in m\n",
    "    'pixel_sz_y': 4.5e-6  # in m\n",
    "}\n",
    "stereo_params = {\n",
    "    'baseline': 412e-3,  # in m\n",
    "    'center_dist_to_obj': 639e-3,  # in m\n",
    "    'cam1_dist_to_obj': 660e-3,  # in m\n",
    "    'cam2_dist_to_obj': 641e-3  # in m\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вручную отметил точку на маркерных досках, т.к. иначе получить координаты всех точек на маркерной доске, чтобы они формировали матрицу не удавалось (пробовал это сделать с помощью выделения Blob-ов, binary threshold, Hough Circle Transform, использовать модели на нейронных сетях - U-Net). Выделить маркеры в целом получалось, однако затем всё равно требуется вручную сопоставить их с координатами в системе координат относительно маркерной доски.\n",
    "\n",
    "Итак, координаты маркерных точек находятся в файле *markers_coords.json*. На каждом изображении 49 точек, формирующих матрицу 7 на 7. Точки затем сопоставляются с координатами самой доски в реальном 3D пространстве *real_markers_coords*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ВАЖНО__  \n",
    "'cam1_imgs_paths' - должен соответствовать пути к папке, в которой лежат изображения с левой камеры  \n",
    "'cam2_imgs_paths' - соответственно, с правой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_folder = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                              'test_imgs')\n",
    "cam1_imgs_paths = glob.glob(f\"{imgs_folder}/cam2_*.jpg\")\n",
    "cam2_imgs_paths = glob.glob(f\"{imgs_folder}/cam1_*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam1_imgs = {}\n",
    "cam2_imgs = {}\n",
    "\n",
    "for img_name in sorted(cam1_imgs_paths):\n",
    "    img = cv.imread(img_name, 0)\n",
    "    img_name = os.path.splitext(os.path.basename(img_name))[0]\n",
    "    cam1_imgs[img_name] = img\n",
    "\n",
    "for img_name in sorted(cam2_imgs_paths):\n",
    "    img = cv.imread(img_name, 0)\n",
    "    img_name = os.path.splitext(os.path.basename(img_name))[0]\n",
    "    cam2_imgs[img_name] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20)) \n",
    "for i, img in enumerate(cam1_imgs.values()):\n",
    "    plt.subplot(1, len(cam1_imgs), i + 1) \n",
    "    plt.imshow(cv.cvtColor(img, cv.COLOR_GRAY2RGB))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20)) \n",
    "for i, img in enumerate(cam2_imgs.values()):\n",
    "    plt.subplot(1, len(cam2_imgs), i + 1) \n",
    "    plt.imshow(cv.cvtColor(img, cv.COLOR_GRAY2RGB))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь, в котором будет определено расстояние между маркерами для каждого изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_to_markers_dist = {}\n",
    "\n",
    "for img_name in cam1_imgs.keys():\n",
    "    if img_name == 'cam2_4':\n",
    "        img_name_to_markers_dist[img_name] = S_MARKERS_DIST\n",
    "    else:\n",
    "        img_name_to_markers_dist[img_name] = M_MARKERS_DIST\n",
    "\n",
    "for img_name in cam2_imgs.keys():\n",
    "    if img_name == 'cam1_4':\n",
    "        img_name_to_markers_dist[img_name] = S_MARKERS_DIST\n",
    "    else:\n",
    "        img_name_to_markers_dist[img_name] = M_MARKERS_DIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка координат маркерных точек для калибровочных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the markers points for these images\n",
    "with open('data/markers_coords.json', 'r') as file:\n",
    "    markers_coords = json.load(file)\n",
    "\n",
    "# Convert the string coordinates (\"x, y\") into tuples of integers\n",
    "for img_name, coords in markers_coords.items():\n",
    "    markers_coords[img_name] = [tuple(map(int, point.split(','))) for point in coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_w_markers(img, markers):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv.cvtColor(img, cv.COLOR_GRAY2RGB))\n",
    "    for marker_coords in np.array(markers, dtype=np.float32):\n",
    "        plt.scatter(marker_coords[0], marker_coords[1], color='red', s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример размеченной фотки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = 'cam2_5'\n",
    "img = cam1_imgs[img_name]\n",
    "plot_img_w_markers(img, markers_coords[img_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск внутренних параметров камер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление camera matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camera matrix можно вычислить исходя из следующих параметров:\n",
    "1. фокусное расстояние: 80mm\n",
    "2. размер пикселя: 4.5 µm × 4.5 µm\n",
    "3. разрешение изображения: 5120 × 4096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_CM(focal_length_m, img_resolution, pixel_sz_x, pixel_sz_y):\n",
    "    width, height = img_resolution\n",
    "\n",
    "    f_x = focal_length_m / pixel_sz_x\n",
    "    f_y = focal_length_m / pixel_sz_y\n",
    "\n",
    "    c_x = width / 2.0\n",
    "    c_y = height / 2.0\n",
    "\n",
    "    CM = np.array([\n",
    "        [f_x, 0, c_x],\n",
    "        [0, f_y, c_y],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    return CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = calculate_CM(**camera_params)\n",
    "CM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск вектора искажений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас уже есть матрица камеры и вычислим коэффициенты искажения с использованием калибровочных изображений и отмеченных вручную маркерах на них (*test_imgs/* и *markers_coords.json*).\n",
    "\n",
    "calibrateCamera принимает начальное предположение для матрицы камеры (CM_initial_guess) и вычисляет только вектор искажения при использовании определенных флагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distortion(imgs, img_resolution, \n",
    "                         markers_coords, markers_dist, grid_shape,\n",
    "                         CM, dist_initial_guess=None):\n",
    "    rows, columns = grid_shape\n",
    "\n",
    "    real_markers = []  # 3D markers coords in real world for each image\n",
    "    img_markers = []  # 2D markers coords in image plane for each image\n",
    "    \n",
    "    for img_name in imgs:\n",
    "        # Get the 2D image marker coordinates\n",
    "        markers = np.array(markers_coords[img_name], dtype=np.float32)\n",
    "        img_markers.append(markers)\n",
    "\n",
    "        # Create 3D real-world coordinates grid based on the specific markers_dist for this image\n",
    "        real_markers_grid_coords = np.zeros((rows * columns, 3), np.float32)\n",
    "        real_markers_grid_coords[:, :2] = np.mgrid[0:rows, 0:columns].T.reshape(-1, 2)\n",
    "        real_markers_grid_coords *= markers_dist[img_name]  # Apply specific markers_dist for this image\n",
    "    \n",
    "        real_markers.append(real_markers_grid_coords)\n",
    "\n",
    "    # Calibration flags: use initial guesses, fix focal length and principal point\n",
    "    flags = cv.CALIB_USE_INTRINSIC_GUESS + cv.CALIB_FIX_FOCAL_LENGTH + cv.CALIB_FIX_PRINCIPAL_POINT\n",
    "    ret, _, dist, _, _ = cv.calibrateCamera(real_markers, img_markers, img_resolution, \n",
    "                                            CM, dist_initial_guess, \n",
    "                                            flags=flags)\n",
    "\n",
    "    print(f'RMSE: {ret}')\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cam_imgs = {**cam1_imgs, **cam2_imgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = calculate_distortion(\n",
    "    all_cam_imgs, camera_params['img_resolution'], \n",
    "    markers_coords, img_name_to_markers_dist, GRID_SHAPE, \n",
    "    CM, np.array([0.01, 0, 0, 0, 0])\n",
    ")\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор искажений не похаж на правду, слишком экстремальные значения, будем исопльзовать кастомный вектор, оцененный из параметров объектива"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.array([0, 0, 0, 0, 0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск всех внутренних параметров с помощью OpenCV calibrateCamera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intrinsic(imgs, img_resolution, \n",
    "                         markers_coords, markers_dist, grid_shape,\n",
    "                         CM_initial_guess, dist_initial_guess):\n",
    "    rows, columns = grid_shape\n",
    "\n",
    "    real_markers = []  # 3D markers coords in real world for each image\n",
    "    img_markers = []  # 2D markers coords in image plane for each image\n",
    "    \n",
    "    for img_name in imgs:\n",
    "        # Get the 2D image marker coordinates\n",
    "        markers = np.array(markers_coords[img_name], dtype=np.float32)\n",
    "        img_markers.append(markers)\n",
    "\n",
    "        # Create 3D real-world coordinates grid based on the specific markers_dist for this image\n",
    "        real_markers_grid_coords = np.zeros((rows * columns, 3), np.float32)\n",
    "        real_markers_grid_coords[:, :2] = np.mgrid[0:rows, 0:columns].T.reshape(-1, 2)\n",
    "        real_markers_grid_coords *= markers_dist[img_name]  # Apply specific markers_dist for this image\n",
    "    \n",
    "        real_markers.append(real_markers_grid_coords)\n",
    "\n",
    "    # Calibration flags: use initial guesses, fix focal length and principal point\n",
    "    flags = cv.CALIB_USE_INTRINSIC_GUESS\n",
    "    ret, CM, dist, _, _ = cv.calibrateCamera(real_markers, img_markers, img_resolution, \n",
    "                                            CM_initial_guess, dist_initial_guess, \n",
    "                                            flags=flags)\n",
    "\n",
    "    print(f'RMSE: {ret}')\n",
    "    return CM, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM, dist = calculate_intrinsic(\n",
    "    all_cam_imgs, camera_params['img_resolution'], \n",
    "    markers_coords, img_name_to_markers_dist, GRID_SHAPE, \n",
    "    CM, dist)\n",
    "\n",
    "CM, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стерео-калибровка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual estimation of R, T matrices given the organizator info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rotation_matrix(rx_deg, ry_deg, rz_deg):\n",
    "    rx_rad = np.deg2rad(rx_deg)\n",
    "    ry_rad = np.deg2rad(ry_deg)\n",
    "    rz_rad = np.deg2rad(rz_deg)\n",
    "    \n",
    "    R_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(rx_rad), -np.sin(rx_rad)],\n",
    "        [0, np.sin(rx_rad), np.cos(rx_rad)]\n",
    "    ])\n",
    "    R_y = np.array([\n",
    "        [np.cos(ry_rad), 0, np.sin(ry_rad)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(ry_rad), 0, np.cos(ry_rad)]\n",
    "    ])\n",
    "    R_z = np.array([\n",
    "        [np.cos(rz_rad), -np.sin(rz_rad), 0],\n",
    "        [np.sin(rz_rad), np.cos(rz_rad), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    R = np.matmul(np.matmul(R_z, R_y), R_x)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating R\n",
    "# Set global coordinate system in between cameras\n",
    "R1 = create_rotation_matrix(0, 18.5, 0)\n",
    "R2 = create_rotation_matrix(0, -18.5, 0)\n",
    "\n",
    "R = np.dot(R2, R1.T)\n",
    "\n",
    "# Calculating T\n",
    "C1 = np.array([-0.206, 0.0, 0.0]) # Center of cam1 coord system\n",
    "C2 = np.array([0.206, 0.0, 0.0])  # Center of cam2 coord system\n",
    "\n",
    "# Translation vector\n",
    "t12_world = C2 - C1\n",
    "\n",
    "T = np.dot(R1.T, t12_world)\n",
    "\n",
    "R, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_calibrate(cam1_imgs, cam2_imgs, img_resolution, \n",
    "                     markers_coords, markers_dist, grid_shape, \n",
    "                     CM, dist,\n",
    "                     R_guess=None, T_guess=None):\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 1000, 1e-6)\n",
    "    \n",
    "    rows, columns = grid_shape\n",
    "\n",
    "    real_markers = []  # 3D points in real world space\n",
    "    cam1_img_markers = []\n",
    "    cam2_img_markers = []\n",
    "\n",
    "    for cam1_img_name, cam2_img_name in zip(cam1_imgs, cam2_imgs):\n",
    "        cam1_markers = markers_dist[cam1_img_name]\n",
    "\n",
    "        # 3D world coordinates for each grid point based on markers distance for each image\n",
    "        real_markers_grid_coords = np.zeros((rows * columns, 3), np.float32)\n",
    "        real_markers_grid_coords[:, :2] = np.mgrid[0:rows, 0:columns].T.reshape(-1, 2)\n",
    "        real_markers_grid_coords *= cam1_markers\n",
    "\n",
    "        real_markers.append(real_markers_grid_coords)\n",
    "\n",
    "        cam1_img_markers.append(np.array(markers_coords[cam1_img_name], dtype=np.float32))\n",
    "        cam2_img_markers.append(np.array(markers_coords[cam2_img_name], dtype=np.float32))\n",
    "\n",
    "    stereocalibration_flags = cv.CALIB_FIX_INTRINSIC + cv.CALIB_USE_EXTRINSIC_GUESS\n",
    "    \n",
    "    ret, _, _, _, _, R, T, E, F, _, _, _ = cv.stereoCalibrateExtended(\n",
    "        real_markers, \n",
    "        cam1_img_markers, \n",
    "        cam2_img_markers, \n",
    "        CM, dist,\n",
    "        CM, dist,\n",
    "        img_resolution,\n",
    "        R_guess, T_guess,\n",
    "        criteria=criteria, \n",
    "        flags=stereocalibration_flags\n",
    "    )\n",
    " \n",
    "    print(f\"Reprojection Error (RMSE): {ret}\")\n",
    "    return R, T, E, F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: The relative rotation matrix between the first and second cameras. This matrix describes how the second camera is rotated relative to the first camera.\n",
    "\n",
    "T: The translation vector between the first and second cameras. This vector describes how far and in which direction the second camera is from the first camera.\n",
    "\n",
    "E: The essential matrix. This matrix encodes the rotation and translation between the two cameras, and it can be used to compute the epipolar geometry (constraints for stereo matching).\n",
    "\n",
    "F: The fundamental matrix. This is another way of representing the geometric relationship between the two cameras. It defines the epipolar lines in stereo matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, T, E, F = stereo_calibrate(cam1_imgs, cam2_imgs, camera_params['img_resolution'],\n",
    "                              markers_coords, img_name_to_markers_dist, GRID_SHAPE,\n",
    "                              CM, dist, R, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим baseline и углы поворота для полученных матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_to_euler_angles(R):\n",
    "    sy = np.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)\n",
    "    \n",
    "    singular = sy < 1e-6\n",
    "    \n",
    "    if not singular:\n",
    "        yaw = np.arctan2(R[2, 1], R[2, 2])\n",
    "        pitch = np.arctan2(-R[2, 0], sy)\n",
    "        roll = np.arctan2(R[1, 0], R[0, 0])\n",
    "    else:\n",
    "        yaw = np.arctan2(-R[1, 2], R[1, 1])\n",
    "        pitch = np.arctan2(-R[2, 0], sy)\n",
    "        roll = 0\n",
    "\n",
    "    return np.degrees(np.array([yaw, pitch, roll]))\n",
    "\n",
    "def show_baseline_and_rotation(T_calculated, R_calculated, real_baseline, R_real=None):\n",
    "    calculated_baseline = np.linalg.norm(T)\n",
    "    \n",
    "    print(f\"Calculated baseline: {calculated_baseline:.2f} m\")\n",
    "    print(f\"Real baseline: {real_baseline:.2f} m\")\n",
    "    \n",
    "    baseline_difference = np.abs(calculated_baseline - real_baseline)\n",
    "    print(f\"Baseline difference: {baseline_difference:.2f} m\")\n",
    "    \n",
    "    euler_angles_calculated = rotation_matrix_to_euler_angles(R_calculated)\n",
    "    print(\"\\nCalculated Euler angles (Yaw, Pitch, Roll):\")\n",
    "    print(f\"Yaw: {euler_angles_calculated[0]:.2f} degrees\")\n",
    "    print(f\"Pitch: {euler_angles_calculated[1]:.2f} degrees\")\n",
    "    print(f\"Roll: {euler_angles_calculated[2]:.2f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_baseline_and_rotation(T, R, stereo_params['baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the camera's intrinsic and extrinsic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data_json = {\n",
    "    'CM': CM.tolist(),\n",
    "    'dist': dist.tolist(),\n",
    "    'R': R.tolist(),\n",
    "    'T': T.tolist(),\n",
    "    'E': E.tolist(),\n",
    "    'F': F.tolist()\n",
    "}\n",
    "\n",
    "with open('data/calibration_data_real.json', 'w') as f:\n",
    "    json.dump(calibration_data_json, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-sev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
