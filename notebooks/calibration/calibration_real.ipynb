{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Калибровка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых модулей/функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from pixelpoint.calibration import load_images, load_marker_coords, save_calibration_params, calculate_CM, calculate_RT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Константы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_cam1_ - левая камера  \n",
    "_cam2_ - правая камера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_params = {\n",
    "    's_markers_dist': 33e-3 / 6.0,  # real distance between markers in small calibration plane (in m)\n",
    "    'm_markers_dist': 67e-3 / 6.0,  # real distance between markers in medium calibration plane (in m)\n",
    "    'grid_shape': (7, 7)\n",
    "}\n",
    "camera_params = {\n",
    "    'focal_length_m': 80e-3,\n",
    "    'img_resolution': (5120, 4096),\n",
    "    'pixel_sz_x': 4.5e-6,  # in m\n",
    "    'pixel_sz_y': 4.5e-6  # in m\n",
    "}\n",
    "stereo_params = {\n",
    "    'baseline': 412e-3,  # in m\n",
    "    'center_dist_to_obj': 639e-3,  # in m\n",
    "    'cam_dist_to_obj': 650e-3,  # in m\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вручную отметил координаты маркерных точек на досках, т.к. иначе получить координаты всех точек на маркерной доске, чтобы они формировали матрицу не удавалось (пробовал это сделать с помощью выделения Blob-ов, binary threshold, Hough Circle Transform, использовать модели на нейронных сетях - U-Net). Выделить маркеры в целом получалось, однако затем всё равно требуется вручную сопоставить их с координатами в системе координат относительно маркерной доски.\n",
    "\n",
    "Итак, координаты маркерных точек находятся в файле *markers_coords.json*. На каждом изображении 49 точек, формирующих матрицу 7 на 7. Точки затем сопоставляются с координатами самой доски в реальном 3D пространстве *real_markers_coords*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_folder = Path('calibration_images_markers')\n",
    "cam1_imgs, cam2_imgs = load_images(imgs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10)) \n",
    "for i, img in enumerate(cam1_imgs.values()):\n",
    "    plt.subplot(1, len(cam1_imgs), i + 1) \n",
    "    plt.imshow(cv.cvtColor(img, cv.COLOR_GRAY2RGB))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10)) \n",
    "for i, img in enumerate(cam2_imgs.values()):\n",
    "    plt.subplot(1, len(cam2_imgs), i + 1) \n",
    "    plt.imshow(cv.cvtColor(img, cv.COLOR_GRAY2RGB))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь, в котором будет определено расстояние между маркерами для каждого изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_to_markers_dist = {}\n",
    "\n",
    "for img_name in cam1_imgs.keys():\n",
    "    if img_name == 'cam2_4':\n",
    "        img_name_to_markers_dist[img_name] = pattern_params['s_markers_dist']\n",
    "    else:\n",
    "        img_name_to_markers_dist[img_name] = pattern_params['m_markers_dist']\n",
    "\n",
    "for img_name in cam2_imgs.keys():\n",
    "    if img_name == 'cam1_4':\n",
    "        img_name_to_markers_dist[img_name] = pattern_params['s_markers_dist']\n",
    "    else:\n",
    "        img_name_to_markers_dist[img_name] = pattern_params['m_markers_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка координат маркерных точек для калибровочных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_file_path = Path('calibration_params/markers_coords.json')\n",
    "markers_coords = load_marker_coords(markers_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_w_markers(img, markers):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv.cvtColor(img, cv.COLOR_GRAY2RGB))\n",
    "    for marker_coords in np.array(markers, dtype=np.float32):\n",
    "        plt.scatter(marker_coords[0], marker_coords[1], color='red', s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример размеченного изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = 'cam2_4'\n",
    "img = cam1_imgs[img_name]\n",
    "plot_img_w_markers(img, markers_coords[img_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск внутренних параметров камер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление camera matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camera matrix можно вычислить исходя из следующих параметров:\n",
    "1. фокусное расстояние: 80mm\n",
    "2. размер пикселя: 4.5 µm × 4.5 µm\n",
    "3. разрешение изображения: 5120 × 4096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = calculate_CM(**camera_params)\n",
    "print(f'Camera matrix:\\n{CM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск вектора искажений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас уже есть матрица камеры, вычислим коэффициенты искажения с использованием калибровочных изображений и координат маркерных точек на них (*calibration_images_markers/* и *markers_coords.json*).\n",
    "\n",
    "calibrateCamera принимает начальное предположение для матрицы камеры (CM_initial_guess) и вычисляет только вектор искажения при использовании определенных флагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distortion(imgs, img_resolution, \n",
    "                         markers_coords, markers_dist, grid_shape,\n",
    "                         CM, dist_initial_guess=None):\n",
    "    rows, columns = grid_shape\n",
    "\n",
    "    real_markers = []  # 3D markers coords in real world for each image\n",
    "    img_markers = []  # 2D markers coords in image plane for each image\n",
    "    \n",
    "    for img_name in imgs:\n",
    "        # Get the 2D image marker coordinates\n",
    "        markers = np.array(markers_coords[img_name], dtype=np.float32)\n",
    "        img_markers.append(markers)\n",
    "\n",
    "        # Create 3D real-world coordinates grid based on the specific markers_dist for this image\n",
    "        real_markers_grid_coords = np.zeros((rows * columns, 3), np.float32)\n",
    "        real_markers_grid_coords[:, :2] = np.mgrid[0:rows, 0:columns].T.reshape(-1, 2)\n",
    "        real_markers_grid_coords *= markers_dist[img_name]\n",
    "    \n",
    "        real_markers.append(real_markers_grid_coords)\n",
    "\n",
    "    # Calibration flags: use initial guesses, fix focal length and principal point\n",
    "    flags = cv.CALIB_USE_INTRINSIC_GUESS + cv.CALIB_FIX_FOCAL_LENGTH + cv.CALIB_FIX_PRINCIPAL_POINT\n",
    "    ret, _, dist, _, _ = cv.calibrateCamera(real_markers, img_markers, img_resolution, \n",
    "                                            CM, dist_initial_guess, \n",
    "                                            flags=flags)\n",
    "\n",
    "    print(f'RMSE: {ret}')\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = calculate_distortion(\n",
    "    {**cam1_imgs, **cam2_imgs}, camera_params['img_resolution'], \n",
    "    markers_coords, img_name_to_markers_dist, pattern_params['grid_shape'], \n",
    "    CM, np.array([0., 0., 0., 0., 0.])\n",
    ")\n",
    "print(f'Distortion vector:\\n{dist}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск всех внутренних параметров с помощью OpenCV calibrateCamera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При ситуации, когда мы не знаем никаких параметров камеры: фокусное расстояние, размеры пикселя. Можно получить оценку внутренних параметров с помощью *cv2.calibrateCamera*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intrinsic(imgs, img_resolution, \n",
    "                         markers_coords, markers_dist, grid_shape):\n",
    "    rows, columns = grid_shape\n",
    "\n",
    "    real_markers = []  # 3D markers coords in real world for each image\n",
    "    img_markers = []  # 2D markers coords in image plane for each image\n",
    "    \n",
    "    for img_name in imgs:\n",
    "        # Get the 2D image marker coordinates\n",
    "        markers = np.array(markers_coords[img_name], dtype=np.float32)\n",
    "        img_markers.append(markers)\n",
    "\n",
    "        # Create 3D real-world coordinates grid based on the specific markers_dist for this image\n",
    "        real_markers_grid_coords = np.zeros((rows * columns, 3), np.float32)\n",
    "        real_markers_grid_coords[:, :2] = np.mgrid[0:rows, 0:columns].T.reshape(-1, 2)\n",
    "        real_markers_grid_coords *= markers_dist[img_name]\n",
    "    \n",
    "        real_markers.append(real_markers_grid_coords)\n",
    "\n",
    "    ret, CM, dist, _, _ = cv.calibrateCamera(real_markers, img_markers, img_resolution, None, None)\n",
    "\n",
    "    print(f'RMSE: {ret}')\n",
    "    return CM, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM, dist = calculate_intrinsic(\n",
    "    {**cam1_imgs, **cam2_imgs}, camera_params['img_resolution'], \n",
    "    markers_coords, img_name_to_markers_dist, pattern_params['grid_shape'])\n",
    "\n",
    "print(f'Camera matrix:\\n{CM}')\n",
    "print(f'Distortion vector:\\n{dist}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стерео-калибровка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual estimation of extrinsic parameters for a given optical system configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем матрицы поворота и смещения двух систем координат камер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, T = calculate_RT(stereo_params['baseline'], stereo_params['cam_dist_to_obj'])\n",
    "\n",
    "print(f'Rotation matrix:\\n{R}')\n",
    "print(f'Translation vector:\\n{T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stereo-calibration using the calibration images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_calibrate(cam1_imgs, cam2_imgs, img_resolution, \n",
    "                     markers_coords, markers_dist, grid_shape, \n",
    "                     CM, dist,\n",
    "                     R_guess=None, T_guess=None):\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 1000, 1e-6)\n",
    "    \n",
    "    rows, columns = grid_shape\n",
    "\n",
    "    real_markers = []  # 3D points in real world space\n",
    "    cam1_img_markers = []\n",
    "    cam2_img_markers = []\n",
    "\n",
    "    for cam1_img_name, cam2_img_name in zip(cam1_imgs, cam2_imgs):\n",
    "        cam1_markers = markers_dist[cam1_img_name]\n",
    "\n",
    "        # 3D world coordinates for each grid point based on markers distance for each image\n",
    "        real_markers_grid_coords = np.zeros((rows * columns, 3), np.float32)\n",
    "        real_markers_grid_coords[:, :2] = np.mgrid[0:rows, 0:columns].T.reshape(-1, 2)\n",
    "        real_markers_grid_coords *= cam1_markers\n",
    "\n",
    "        real_markers.append(real_markers_grid_coords)\n",
    "\n",
    "        cam1_img_markers.append(np.array(markers_coords[cam1_img_name], dtype=np.float32))\n",
    "        cam2_img_markers.append(np.array(markers_coords[cam2_img_name], dtype=np.float32))\n",
    "\n",
    "    stereocalibration_flags = cv.CALIB_FIX_INTRINSIC + cv.CALIB_USE_EXTRINSIC_GUESS\n",
    "    \n",
    "    ret, _, _, _, _, R, T, E, F, _, _, _ = cv.stereoCalibrateExtended(\n",
    "        real_markers, \n",
    "        cam1_img_markers, \n",
    "        cam2_img_markers, \n",
    "        CM, dist,\n",
    "        CM, dist,\n",
    "        img_resolution,\n",
    "        R_guess, T_guess,\n",
    "        criteria=criteria, \n",
    "        flags=stereocalibration_flags\n",
    "    )\n",
    " \n",
    "    print(f\"Reprojection Error (RMSE): {ret}\")\n",
    "    return R, T, E, F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: The relative rotation matrix between the first and second cameras. This matrix describes how the second camera is rotated relative to the first camera.\n",
    "\n",
    "T: The translation vector between the first and second cameras. This vector describes how far and in which direction the second camera is from the first camera.\n",
    "\n",
    "E: The essential matrix. This matrix encodes the rotation and translation between the two cameras, and it can be used to compute the epipolar geometry (constraints for stereo matching).\n",
    "\n",
    "F: The fundamental matrix. This is another way of representing the geometric relationship between the two cameras. It defines the epipolar lines in stereo matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, T, E, F = stereo_calibrate(cam1_imgs, cam2_imgs, camera_params['img_resolution'],\n",
    "                              markers_coords, img_name_to_markers_dist, pattern_params['grid_shape'],\n",
    "                              CM, dist, R, T)\n",
    "\n",
    "print(f'Rotation matrix:\\n{R}')\n",
    "print(f'Translation vector:\\n{T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим baseline и углы поворота для полученных матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_to_euler_angles(R):\n",
    "    sy = np.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)\n",
    "    \n",
    "    singular = sy < 1e-6\n",
    "    \n",
    "    if not singular:\n",
    "        yaw = np.arctan2(R[2, 1], R[2, 2])\n",
    "        pitch = np.arctan2(-R[2, 0], sy)\n",
    "        roll = np.arctan2(R[1, 0], R[0, 0])\n",
    "    else:\n",
    "        yaw = np.arctan2(-R[1, 2], R[1, 1])\n",
    "        pitch = np.arctan2(-R[2, 0], sy)\n",
    "        roll = 0\n",
    "\n",
    "    return np.degrees(np.array([yaw, pitch, roll]))\n",
    "\n",
    "def show_baseline_and_rotation(T_calculated, R_calculated, real_baseline, R_real=None):\n",
    "    calculated_baseline = np.linalg.norm(T)\n",
    "    \n",
    "    print(f\"Calculated baseline: {calculated_baseline:.2f} m\")\n",
    "    print(f\"Real baseline: {real_baseline:.2f} m\")\n",
    "    \n",
    "    baseline_difference = np.abs(calculated_baseline - real_baseline)\n",
    "    print(f\"Baseline difference: {baseline_difference:.2f} m\")\n",
    "    \n",
    "    euler_angles_calculated = rotation_matrix_to_euler_angles(R_calculated)\n",
    "    print(\"\\nCalculated Euler angles (Yaw, Pitch, Roll):\")\n",
    "    print(f\"Yaw: {euler_angles_calculated[0]:.2f} degrees\")\n",
    "    print(f\"Pitch: {euler_angles_calculated[1]:.2f} degrees\")\n",
    "    print(f\"Roll: {euler_angles_calculated[2]:.2f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_baseline_and_rotation(T, R, stereo_params['baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the camera's intrinsic and extrinsic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_calibration_params(CM, dist, R, T, E, F, 'calibration_params/calib_params_real.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-sev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
